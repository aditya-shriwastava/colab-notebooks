{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-shriwastava/colab-notebooks/blob/master/FasterRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "1. [ ] Remove degenerate rois output from RPN."
      ],
      "metadata": {
        "id": "ZW8aAmdFfo75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip3 install"
      ],
      "metadata": {
        "id": "PfxaoBAYX4A2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvEk_2A4eSu5"
      },
      "outputs": [],
      "source": [
        "!pip3 install pytest\n",
        "!pip3 install ipytest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports\n"
      ],
      "metadata": {
        "id": "kEr3zhg5YAPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "\n",
        "import pytest\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "gkTj6qDyWUgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load VOC data"
      ],
      "metadata": {
        "id": "MrpqtVAtXulR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxbT_hKKZI6n"
      },
      "outputs": [],
      "source": [
        "train_data = torchvision.datasets.VOCDetection(\n",
        "  root=\"./datasets\", year=\"2012\",\n",
        "  image_set='train', download=True,\n",
        "  transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "  ])\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
        "\n",
        "val_data = torchvision.datasets.VOCDetection(\n",
        "  root=\"./datasets\", year=\"2012\",\n",
        "  image_set='val', download=True,\n",
        "  transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "  ])\n",
        ")\n",
        "val_loader = DataLoader(val_data, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bbox utils"
      ],
      "metadata": {
        "id": "d2lBCqcRTzUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE6qf3e0YnvZ"
      },
      "outputs": [],
      "source": [
        "def filter_cross_boundary(bboxs, bbox_within):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    bboxs: bboxs tensor in minmax format of shape (N,4) to filter.\n",
        "    bbox_within: bbox tensor in minmax format of shape (4,) within\n",
        "    which bboxs should be.\n",
        "  Returns:\n",
        "    A tensor of index of bboxs which are within bbox_within.\n",
        "  \"\"\"\n",
        "  N = bboxs.shape[0]\n",
        "  corner_offset = bboxs - bbox_within\n",
        "  iswithin = torch.logical_and(\n",
        "    (corner_offset[:,:2] >= 0).all(axis=1),\n",
        "    (corner_offset[:,2:] <= 0).all(axis=1)\n",
        "  )\n",
        "  return torch.arange(N)[iswithin]\n",
        "\n",
        "def filter_iou(bboxs1, bboxs2, min_iou=0, max_iou=1):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    bboxs1: bboxs tensor in xyxy format of shape (N,4)\n",
        "    bboxs2: bboxs tensor in xyxy format of shape (M,4)\n",
        "  Returns:\n",
        "    A tensor of index of bboxs1 such that min_iou <= iou(bboxs[i], bboxs2) <= max_iou\n",
        "    for i in [1,2, ..., N] and argmax_i(iou(bboxs[i], bboxs2))\n",
        "  \"\"\"\n",
        "  N = bboxs1.shape[0]\n",
        "  iou_max = torchvision.ops.box_iou(bboxs1, bboxs2).max(axis=1)[0]\n",
        "  iswithin = torch.logical_and(\n",
        "    (iou_max >= min_iou),\n",
        "    (iou_max <= max_iou)\n",
        "  )\n",
        "  return torch.arange(N, device=device)[iswithin]\n",
        "\n",
        "def sample_bboxs(bboxs, max_bboxs):\n",
        "  N = bboxs.shape[0]\n",
        "  if N > max_bboxs:\n",
        "    index = random.sample(range(N),max_bboxs)\n",
        "    return torch.tensor(index, device=device)\n",
        "  else:\n",
        "    return torch.arange(N, device=device)\n",
        "\n",
        "def get_bboxs(bboxs_offset, anchors):\n",
        "  \"\"\"Reff https://arxiv.org/pdf/1506.01497.pdf eq2\n",
        "  Args:\n",
        "    bboxs_offset: tensor of shape (N,4) representing offset of bboxs in\n",
        "      ctrwh format form anchors.\n",
        "    anchors: tensor of shape (N,4) representing anchors in ctrwh format\n",
        "  Returns:\n",
        "    A tensor of shape (N,4) representing bboxs\n",
        "  \"\"\"\n",
        "  bboxs = torch.zeros_like(bboxs_offset)\n",
        "\n",
        "  bboxs[:,0] = anchors[:,0] + (bboxs_offset[:,0] * anchors[:,2])\n",
        "  bboxs[:,1] = anchors[:,1] + (bboxs_offset[:,1] * anchors[:,3])\n",
        "  bboxs[:,2] = anchors[:,2] * torch.exp(bboxs_offset[:,2])\n",
        "  bboxs[:,3] = anchors[:,3] * torch.exp(bboxs_offset[:,3])\n",
        "  return bboxs\n",
        "\n",
        "def get_bboxs_offset(bboxs, anchors):\n",
        "  \"\"\"Reff https://arxiv.org/pdf/1506.01497.pdf eq2\n",
        "  Args:\n",
        "    bboxs: tensor of shape (N,4) representing bboxs in ctrwh format.\n",
        "    anchors: tensor of shape (N,4) representing anchors in ctrwh format\n",
        "  Returns:\n",
        "    A tensor of shape (N,4) representing bboxs_offset\n",
        "  \"\"\"\n",
        "  bboxs_offset = torch.zeros_like(bboxs)\n",
        "\n",
        "  bboxs_offset[:,0] = (bboxs[:,0] - anchors[:,0]) / anchors[:,2]\n",
        "  bboxs_offset[:,1] = (bboxs[:,1] - anchors[:,1]) / anchors[:,3]\n",
        "  bboxs_offset[:,2] = torch.log(bboxs[:,2] / anchors[:,2])\n",
        "  bboxs_offset[:,3] = torch.log(bboxs[:,3] / anchors[:,3])\n",
        "  return bboxs_offset\n",
        "\n",
        "def get_anchors(\n",
        "    Hf, Wf,\n",
        "    ratios = torch.tensor([0.5, 1.0, 2.0]),\n",
        "    scale = torch.tensor([8.0, 16.0, 32.0]),\n",
        "    base_size = 16,\n",
        "    stride = 16):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    Hf: Height of feature_map we want anchors for.\n",
        "    Wf: Width of feature_map we want anchors for.\n",
        "    ratios: tensor of shape (3,) representing aspect ratios of anchors.\n",
        "    scale: tensor of shape (3,) representing scale of anchors.\n",
        "    base_size: base size of the anchor which will be scaled.\n",
        "    stride: distance by which anchors will be shifted along height and\n",
        "      weidth of feature_map.\n",
        "  Returns:\n",
        "    A tensor of shape (Hf*Wf*9,4) representing 9 anchors in cxcywh format at\n",
        "    each pixel position.\n",
        "  \"\"\"\n",
        "  def get_base_anchors(ratios, scale, base_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      ratios: tensor of shape (3,) representing aspect ratios of anchors.\n",
        "      scale: tensor of shape (3,) representing scale of anchors.\n",
        "      base_size: base size of the anchor which will be scaled.\n",
        "    Returns:\n",
        "      A tensor of shape (9,4) representing 9 anchors (3 ratios * 3 scale)\n",
        "      in cxcywh format.\n",
        "    \"\"\"\n",
        "    # 1. base_anchors(1,4) in ctrwh format\n",
        "    base_anchors = torch.tensor([[\n",
        "      (base_size-1)/2,\n",
        "      (base_size-1)/2,\n",
        "      base_size,\n",
        "      base_size\n",
        "    ]])\n",
        "\n",
        "    # 2. Apply ratios to base_anchors(1,4) to generate base_anchors(3,4)\n",
        "    size = base_anchors[0,2] * base_anchors[0,3]\n",
        "    base_anchors = base_anchors.repeat(3,1)\n",
        "    heights = torch.sqrt(size * ratios)\n",
        "    widths = heights/ratios\n",
        "    base_anchors[:,2] = heights\n",
        "    base_anchors[:,3] = widths\n",
        "\n",
        "    # 3. Apply scale to base_anchors(3,4) to generate base_anchors(9,4)\n",
        "    base_anchors = base_anchors.repeat(1,3).view(9,4)\n",
        "    base_anchors[:,2:4] *= scale.repeat(3).view(9,1).repeat(1,2)\n",
        "\n",
        "    return torch.round(base_anchors)\n",
        "\n",
        "  anchors = get_base_anchors(ratios, scale, base_size)\\\n",
        "    .view(-1)\\\n",
        "    .unsqueeze(0).repeat(Wf,1)\\\n",
        "    .unsqueeze(0).repeat(Hf,1,1)\\\n",
        "    .view(Hf,Wf,9,4)\n",
        "\n",
        "  shift_x = (torch.arange(0, Wf) * stride).repeat(Hf,1)\\\n",
        "    .repeat(9,1,1).permute(1,2,0)\n",
        "  shift_y = (torch.arange(0, Hf) * stride).unsqueeze(1).repeat(1,Wf)\\\n",
        "    .repeat(9,1,1).permute(1,2,0)\n",
        "\n",
        "  anchors[:,:,:,0] += shift_x\n",
        "  anchors[:,:,:,1] += shift_y\n",
        "\n",
        "  anchors = anchors.to(device)\n",
        "  return anchors.reshape(-1,4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test bbox utils"
      ],
      "metadata": {
        "id": "z_5uRi5tT7c6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DHAOKuLZKgx"
      },
      "outputs": [],
      "source": [
        "%%ipytest\n",
        "\n",
        "class TestFilterCrossBoundary:\n",
        "  @pytest.fixture\n",
        "  def bboxs(self):\n",
        "    bboxs = torch.tensor([\n",
        "        [0, 0, 99, 99],\n",
        "        [50, 50, 99, 99],\n",
        "        [-10, 10, 20, 34],\n",
        "        [10, -10, 20, 34],\n",
        "        [0, 0, 0, 0],\n",
        "        [99, 99, 99, 99],\n",
        "        [0, 0, 199, 199],\n",
        "    ], dtype=torch.float32)\n",
        "    return bboxs\n",
        "\n",
        "  def test(self, bboxs):\n",
        "    bbox_within = torch.tensor([0, 0, 99, 99])\n",
        "    iswithin = torch.tensor(\n",
        "      [True, True, False, False, True, True, False]\n",
        "    )\n",
        "    index = filter_cross_boundary(bboxs, bbox_within)\n",
        "    assert (bboxs[index]- bboxs[iswithin]).mean() < 0.01\n",
        "\n",
        "class TestFilterIou:\n",
        "  @pytest.fixture\n",
        "  def bboxs1(self):\n",
        "    bboxs1 = torch.tensor([\n",
        "        [0, 0, 99, 99],\n",
        "        [50, 50, 99, 99],\n",
        "        [75, 25, 124, 74],\n",
        "        [75, 75, 124, 124],\n",
        "        [75, -25, 124, 24]\n",
        "    ], dtype=torch.float32)\n",
        "    return bboxs1\n",
        "\n",
        "  @pytest.fixture\n",
        "  def bboxs2(self):\n",
        "    bboxs2 = torch.tensor([\n",
        "        [0, 0, 99, 99],\n",
        "        [50, 50, 99, 99]\n",
        "    ], dtype=torch.float32)\n",
        "    return bboxs2\n",
        "\n",
        "  # torchvision.ops.box_iou(bboxs1, bboxs2)\n",
        "  #   [[1.0000, 0.2450],\n",
        "  #    [0.2450, 1.0000],\n",
        "  #    [0.1067, 0.1363],\n",
        "  #    [0.0495, 0.1363],\n",
        "  #    [0.0495, 0.0000]])\n",
        "\n",
        "  def test_case1(self, bboxs1, bboxs2):\n",
        "    index = filter_iou(bboxs1, bboxs2, min_iou=0, max_iou=1)\n",
        "    iswithin = torch.tensor(\n",
        "      [True, True, True, True, True]\n",
        "    )\n",
        "    assert (bboxs1[index] - bboxs1[iswithin]).mean() < 0.01\n",
        "\n",
        "  def test_case2(self, bboxs1, bboxs2):\n",
        "    index = filter_iou(bboxs1, bboxs2, min_iou=0.1)\n",
        "    iswithin = torch.tensor(\n",
        "      [True, True, True, True, False]\n",
        "    )\n",
        "    assert (bboxs1[index] - bboxs1[iswithin]).mean() < 0.01\n",
        "\n",
        "  def test_case3(self, bboxs1, bboxs2):\n",
        "    index = filter_iou(bboxs1, bboxs2, max_iou=0.5)\n",
        "    iswithin = torch.tensor(\n",
        "      [False, False, True, True, True]\n",
        "    )\n",
        "    print(index)\n",
        "    assert (bboxs1[index] - bboxs1[iswithin]).mean() < 0.01\n",
        "\n",
        "class TestSampleBboxs:\n",
        "  @pytest.fixture\n",
        "  def bboxs(self):\n",
        "    bboxs = torch.tensor([\n",
        "        [0, 0, 99, 99],\n",
        "        [50, 50, 99, 99],\n",
        "        [-10, 10, 20, 34],\n",
        "        [10, -10, 20, 34],\n",
        "        [0, 0, 0, 0],\n",
        "        [99, 99, 99, 99],\n",
        "        [0, 0, 199, 199],\n",
        "    ], dtype=torch.float32)\n",
        "    return bboxs\n",
        "\n",
        "  def test_case1(self, bboxs):\n",
        "    index = sample_bboxs(bboxs, max_bboxs=3)\n",
        "    assert index.shape[0] == 3\n",
        "\n",
        "  def test_case2(self, bboxs):\n",
        "    index = sample_bboxs(bboxs, max_bboxs=10)\n",
        "    assert index.shape[0] == bboxs.shape[0]\n",
        "\n",
        "class TestGetAnchors:\n",
        "  @pytest.fixture\n",
        "  def anchors(self):\n",
        "    return get_anchors(\n",
        "      2, 3,\n",
        "      ratios = torch.tensor([0.5, 1.0, 2.0]),\n",
        "      scale = torch.tensor([8.0, 16.0, 32.0]),\n",
        "      base_size = 16,\n",
        "      stride = 16\n",
        "    ).reshape((2,3,9,4))\n",
        "\n",
        "  def test_center(self, anchors):\n",
        "    assert anchors[0,0,0,0] == 8\n",
        "    assert anchors[0,0,0,1] == 8\n",
        "\n",
        "    assert anchors[0,1,0,0] == 8 + 16\n",
        "    assert anchors[0,1,0,1] == 8\n",
        "    \n",
        "    assert anchors[0,2,0,0] == 8 + 16 + 16\n",
        "    assert anchors[0,2,0,1] == 8\n",
        "\n",
        "    assert anchors[1,0,0,0] == 8\n",
        "    assert anchors[1,0,0,1] == 8 + 16\n",
        "\n",
        "    assert anchors[1,1,0,0] == 8 + 16\n",
        "    assert anchors[1,1,0,1] == 8 + 16\n",
        "    \n",
        "    assert anchors[1,2,0,0] == 8 + 16 + 16\n",
        "    assert anchors[1,2,0,1] == 8 + 16\n",
        "\n",
        "  def test_same_anchors_hw(self, anchors):\n",
        "    assert (anchors[0,0,:,2:] == anchors[0,1,:,2:]).all()\n",
        "    assert (anchors[1,1,:,2:] == anchors[0,2,:,2:]).all()\n",
        "    assert (anchors[0,2,:,2:] == anchors[1,2,:,2:]).all()\n",
        "    assert (anchors[0,1,:,2:] == anchors[1,2,:,2:]).all()\n",
        "    assert (anchors[1,0,:,2:] == anchors[0,1,:,2:]).all()\n",
        "  \n",
        "  def test_base_anchors(self, anchors):\n",
        "    scale = torch.sqrt(anchors[0,0,:,2] * anchors[0,0,:,3])\n",
        "    ratios = anchors[0,0,:,2] / anchors[0,0,:,3]\n",
        "    assert (ratios[0:3].mean() - 0.5)**2 < 0.01\n",
        "    assert (ratios[3:6].mean() - 1)**2 < 0.01\n",
        "    assert (ratios[6:9].mean() - 2)**2 < 0.01\n",
        "\n",
        "    assert (scale[[0,3,6]].mean() - 128)**2 < 0.1\n",
        "    assert (scale[[1,4,7]].mean() - 256)**2 < 0.1\n",
        "    assert (scale[[2,5,8]].mean() - 512)**2 < 0.1\n",
        "\n",
        "class TestGetBboxs():\n",
        "  def test1(self):\n",
        "    anchor = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox_offset = torch.tensor([[0,0,0,0]], dtype=torch.float32)\n",
        "    assert (bbox - get_bboxs(bbox_offset,  anchor)).mean()**2 < 0.001\n",
        "\n",
        "  def test2(self):\n",
        "    anchor = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox = torch.tensor([[15,25,5,10]], dtype=torch.float32)\n",
        "    bbox_offset = torch.tensor([[1,1,0,0]], dtype=torch.float32)\n",
        "    assert (bbox - get_bboxs(bbox_offset,  anchor)).mean()**2 < 0.001\n",
        "\n",
        "  def test3(self):\n",
        "    anchor = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox = torch.tensor([[15,25,5 * 2.71828,10 * 2.71828]], dtype=torch.float32)\n",
        "    bbox_offset = torch.tensor([[1,1,1,1]], dtype=torch.float32)\n",
        "    assert (bbox - get_bboxs(bbox_offset,  anchor)).mean()**2 < 0.001\n",
        "\n",
        "  def test4(self):\n",
        "    anchor = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox = torch.tensor([[15,35,5 * (2.71828**3),10 * (2.71828**4)]], dtype=torch.float32)\n",
        "    bbox_offset = torch.tensor([[1,2,3,4]], dtype=torch.float32)\n",
        "    assert (bbox - get_bboxs(bbox_offset,  anchor)).mean()**2 < 0.001\n",
        "\n",
        "class TestGetBboxsOffset():\n",
        "  def test1(self):\n",
        "    anchor = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox_offset = torch.tensor([[0,0,0,0]], dtype=torch.float32)\n",
        "    assert (bbox_offset - get_bboxs_offset(bbox,  anchor)).mean()**2 < 0.001\n",
        "\n",
        "  def test2(self):\n",
        "    anchor = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox = torch.tensor([[15,25,5,10]], dtype=torch.float32)\n",
        "    bbox_offset = torch.tensor([[1,1,0,0]], dtype=torch.float32)\n",
        "    assert (bbox_offset - get_bboxs_offset(bbox,  anchor)).mean()**2 < 0.001\n",
        "\n",
        "  def test3(self):\n",
        "    anchor = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox = torch.tensor([[15,25,5 * 2.71828,10 * 2.71828]], dtype=torch.float32)\n",
        "    bbox_offset = torch.tensor([[1,1,1,1]], dtype=torch.float32)\n",
        "    assert (bbox_offset - get_bboxs_offset(bbox,  anchor)).mean()**2 < 0.001\n",
        "\n",
        "  def test4(self):\n",
        "    anchor = torch.tensor([[10,15,5,10]], dtype=torch.float32)\n",
        "    bbox = torch.tensor([[15,35,5 * (2.71828**3),10 * (2.71828**4)]], dtype=torch.float32)\n",
        "    bbox_offset = torch.tensor([[1,2,3,4]], dtype=torch.float32)\n",
        "    assert (bbox_offset - get_bboxs_offset(bbox,  anchor)).mean()**2 < 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh5esKnofA42"
      },
      "source": [
        "# voc utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv8EDbNnXyjJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "voc_labels = (\"background\",\n",
        "  \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\"bottle\",\n",
        "  \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
        "  \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "  \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        ")\n",
        "\n",
        "voc_label_to_id = {k: v for v, k in enumerate(voc_labels)}\n",
        "\n",
        "def get_bboxs_and_labels(meta, from_loader=True):\n",
        "  objects = meta['annotation']['object']\n",
        "  bboxs = []\n",
        "  labels = []\n",
        "  ids = []\n",
        "  for object_ in objects:\n",
        "    label = object_['name']\n",
        "    xmin = object_['bndbox']['xmin']\n",
        "    ymin = object_['bndbox']['ymin']\n",
        "    xmax = object_['bndbox']['xmax']\n",
        "    ymax = object_['bndbox']['ymax']\n",
        "    if from_loader:\n",
        "      label = label[0] \n",
        "      xmin, ymin = int(xmin[0]), int(ymin[0])\n",
        "      xmax, ymax = int(xmax[0]), int(ymax[0])\n",
        "    else:\n",
        "      xmin, ymin = int(xmin), int(ymin)\n",
        "      xmax, ymax = int(xmax), int(ymax)\n",
        "    \n",
        "    labels.append(label)\n",
        "    ids.append(voc_label_to_id[label])\n",
        "    bboxs.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "  return torch.tensor(bboxs, dtype=torch.float32), labels, torch.tensor(ids, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# disp utils"
      ],
      "metadata": {
        "id": "pSfKejf1sEaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def disp(img, bboxs, labels, ax=None):\n",
        "  img = torchvision.transforms.ToPILImage()(img)\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "  ax.imshow(img)\n",
        "  for bbox, label in zip(bboxs, labels):\n",
        "    bbox = torchvision.ops.box_convert(\n",
        "      bbox.unsqueeze(0),\n",
        "      in_fmt='xyxy',\n",
        "      out_fmt='xywh'\n",
        "    ).squeeze()\n",
        "    ax.add_patch(patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor='r', facecolor='none'))\n",
        "    ax.text(bbox[0], bbox[1], label, fontsize = 12, color='black', backgroundcolor='white')\n",
        "  if ax is None:\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Ze6c7mW_sI4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show sample train data"
      ],
      "metadata": {
        "id": "POH-O3vdsWWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axarr = plt.subplots(2,3)\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "for index, (img, meta) in enumerate(train_loader):\n",
        "  i = index // 3\n",
        "  j = index - (i * 3)\n",
        "  bboxs, labels, ids = get_bboxs_and_labels(meta)\n",
        "  disp(img[0], bboxs, labels, ax=axarr[i,j])\n",
        "  if index == 5:\n",
        "    break\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C0IkKbjyscHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RPN"
      ],
      "metadata": {
        "id": "_ylA-IQ-2-qJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSGwHqzkZDzI"
      },
      "outputs": [],
      "source": [
        "class RPN(nn.Module):\n",
        "  def __init__(self, feature_map_channels):\n",
        "    super().__init__()\n",
        "    self.lower_dim_mapping = nn.Conv2d(\n",
        "      feature_map_channels,\n",
        "      512, 3, stride=1, padding=1\n",
        "    )\n",
        "    self.objectness_score = nn.Conv2d(512, 9*2, 1, stride=1, padding=0)\n",
        "    self.rois_offset = nn.Conv2d(512, 9*4, 1, stride=1, padding=0)\n",
        "\n",
        "    torch.nn.init.normal_(self.objectness_score.weight, std=0.01)\n",
        "    torch.nn.init.constant_(self.objectness_score.bias, 0)\n",
        "\n",
        "    torch.nn.init.normal_(self.rois_offset.weight, std=0.01)\n",
        "    torch.nn.init.constant_(self.rois_offset.bias, 0)\n",
        "\n",
        "    self.reg_loss = nn.SmoothL1Loss()\n",
        "    self.cls_loss = nn.CrossEntropyLoss()\n",
        "    \n",
        "  def forward(self, feature_map, image_boundaries, bboxs_gt=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      feature_map: tensor of shape (1, Cf, Hf, Wf).\n",
        "      image_boundaries: tupel (H, W) giving the boundaries of image.\n",
        "      bboxs_gt: tensor of shape (M,4) in xyxy format representing\n",
        "        M ground truth bboxs on the image corresponding to the given\n",
        "        feature_map.\n",
        "    Returns:\n",
        "      rois: tensor of shape (N,4) representing predicted rois within the image\n",
        "        in xyxy format.\n",
        "      reg_loss: regression loss returned only during training.\n",
        "      cls_loss: classification loss returned only during training.\n",
        "    \"\"\"\n",
        "    H, W = image_boundaries\n",
        "    Hf, Wf = feature_map.shape[2:]\n",
        "\n",
        "    # 1. forward pass\n",
        "    feature_map = F.relu(self.lower_dim_mapping(feature_map)) \n",
        "    objectness_score = self.objectness_score(feature_map)\n",
        "    rois_offset = self.rois_offset(feature_map)\n",
        "\n",
        "    # 2. reshape\n",
        "    objectness_score = objectness_score.squeeze(0).permute((1,2,0)).reshape(-1,2)\n",
        "    rois_offset = rois_offset.squeeze(0).permute((1,2,0)).reshape(-1,4)\n",
        "\n",
        "    # 3. get anchors\n",
        "    anchors = get_anchors(Hf,Wf)\n",
        "\n",
        "    # 4. get rois (in xyxy format) from rois_offset.\n",
        "    rois = get_bboxs(rois_offset, anchors)\n",
        "    rois = torchvision.ops.box_convert(rois, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    # 5. Filter out rois using NMS whith iou threshold: 0.7 and select top 500\n",
        "    index = torchvision.ops.batched_nms(\n",
        "      rois,\n",
        "      objectness_score[:,1],\n",
        "      torch.zeros(Hf*Wf*9),\n",
        "      iou_threshold = 0.7\n",
        "    )[:500]\n",
        "\n",
        "    # 6. Clip rois to image boundaries\n",
        "    \"\"\"\n",
        "    Edge case:\n",
        "      1. What if some bbox is compleatly outside image boundary.?\n",
        "      2. What if for some bboxs clipped version is degenerate ?\n",
        "        (i.e. very small, very thin etc.)\n",
        "    \"\"\"\n",
        "    rois_clipped = torchvision.ops.clip_boxes_to_image(\n",
        "      rois[index],\n",
        "      image_boundaries\n",
        "    )\n",
        "\n",
        "    if not self.training:\n",
        "      return rois_clipped\n",
        "    else:\n",
        "      debug_msg = {}\n",
        "      debug_msg[\"rois\"] = len(rois)\n",
        "      index = torch.arange(len(rois), device=device)\n",
        "\n",
        "      ## 1. Based on iou of rois with bboxs_gt classify rois\n",
        "      ##    into positive and negative.\n",
        "      index_positive = index[torch.hstack((\n",
        "        filter_iou(rois[index], bboxs_gt, min_iou=0.7),\n",
        "        torchvision.ops.box_iou(\n",
        "          rois[index],\n",
        "          bboxs_gt\n",
        "        ).max(axis=1)[0].max(axis=0)[1]\n",
        "      ))]\n",
        "      index_negative = index[filter_iou(rois[index], bboxs_gt, max_iou=0.3)]\n",
        "\n",
        "      ## 2. Sample 256 rois with #rois_positive/#rois_negative\n",
        "      ##    as close to 1 a possible.\n",
        "      index_positive = index_positive[sample_bboxs(\n",
        "        rois[index_positive],\n",
        "        max_bboxs = 128\n",
        "      )]\n",
        "      debug_msg[\"positive_rois\"] = len(index_positive)\n",
        "\n",
        "      index_negative = index_negative[sample_bboxs(\n",
        "        rois[index_negative],\n",
        "        max_bboxs = 256 - index_positive.shape[0]\n",
        "      )]\n",
        "      debug_msg[\"negative_rois\"] = len(index_negative)\n",
        "\n",
        "      ## 3. Calculate cls_loss\n",
        "      target = torch.hstack((\n",
        "        torch.zeros(index_negative.shape[0], dtype=torch.long),\n",
        "        torch.ones(index_positive.shape[0], dtype=torch.long)    \n",
        "      ))\n",
        "      target = target.to(device)\n",
        "      objectness_score_training = torch.vstack((\n",
        "        objectness_score[index_negative],\n",
        "        objectness_score[index_positive]\n",
        "      ))\n",
        "      cls_loss = self.cls_loss(\n",
        "          objectness_score_training,\n",
        "          target\n",
        "      )\n",
        "\n",
        "      ## 4. Calculate index_positive_gt\n",
        "      index_positive_gt = torchvision.ops.box_iou(\n",
        "        rois[index_positive],\n",
        "        bboxs_gt\n",
        "      ).max(axis=1)[1]\n",
        "\n",
        "      ## 5. Calculate reg_loss\n",
        "      bboxs_gt_positive = torchvision.ops.box_convert(\n",
        "        bboxs_gt[index_positive_gt],\n",
        "        in_fmt='xyxy',\n",
        "        out_fmt='cxcywh'\n",
        "      )\n",
        "      bboxs_gt_positive_offset = get_bboxs_offset(\n",
        "        bboxs_gt_positive,\n",
        "        anchors[index_positive]\n",
        "      )\n",
        "      reg_loss = self.reg_loss(\n",
        "        rois_offset[index_positive],\n",
        "        bboxs_gt_positive_offset\n",
        "      )\n",
        "\n",
        "      return rois_clipped,reg_loss, cls_loss, debug_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fast RCNN"
      ],
      "metadata": {
        "id": "_9afzWcx3LGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FastRCNN(nn.Module):\n",
        "  def __init__(self, feature_map_channels):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "      nn.Linear(feature_map_channels*7*7, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, 512),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "    self.cls_score = nn.Linear(512, 21)\n",
        "    self.detections_offset = nn.Linear(512, 21 * 4)\n",
        "    self.reg_loss = nn.SmoothL1Loss()\n",
        "    self.cls_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, rois, feature_map, image_boundaries, gt=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      rois: tensor of shape (N,4) representing region of interests within the \n",
        "        image in xyxy format.\n",
        "      feature_map: tensor of shape (1, Cf, Hf, Wf).\n",
        "      image_boundaries: tupel (H, W) giving the boundaries of image.\n",
        "      gt: tuple (bboxs_gt, labels_gt)\n",
        "        bboxs_gt: tensor of shape (M,4) in xyxy format representing\n",
        "          M ground truth bboxs on the image corresponding to the given\n",
        "          feature_map.\n",
        "        labels_gt: tensor of shape (M,) representing labels corresponding to\n",
        "          bboxs_gt.\n",
        "    Returns (During Training):\n",
        "      reg_loss: regression loss\n",
        "      cls_loss: classification loss\n",
        "    Returns (During Evaluation):\n",
        "      detections: tensor of shape (L,4) representing L detections on the\n",
        "        image corresponding to the feature_map.\n",
        "      labels: tensor of shape (L,) representing predicted labels corresponding\n",
        "      to detections.\n",
        "    \"\"\"\n",
        "    H, W = image_boundaries\n",
        "    Cf, Hf, Wf = feature_map.shape[1:]\n",
        "\n",
        "    # 1. ROI Projection\n",
        "    rois_proj = rois * torch.tensor([Wf/W,Hf/H,Wf/W,Hf/H], device=device)\n",
        "    \n",
        "    # 2. ROI Pooling\n",
        "    rois_fm = torchvision.ops.roi_pool(\n",
        "      feature_map,\n",
        "      [rois_proj],\n",
        "      7\n",
        "    ).reshape(-1,Cf*7*7)\n",
        "\n",
        "    # 3. forward pass\n",
        "    rois_fm = self.fc(rois_fm)\n",
        "    cls_score = self.cls_score(rois_fm)\n",
        "    detections_offset = self.detections_offset(rois_fm).reshape(-1,21,4)\n",
        "\n",
        "    if self.training:\n",
        "      bboxs_gt = gt['bboxs_gt']\n",
        "      labels_gt = gt['labels_gt']\n",
        "      debug_msg = {}\n",
        "      \n",
        "      ## 1. Based on iou of rois with bboxs_gt classify rois into\n",
        "      #     positive and negative.\n",
        "      index_positive = torch.hstack((\n",
        "        filter_iou(rois, bboxs_gt, min_iou=0.5),\n",
        "        torchvision.ops.box_iou(\n",
        "          rois,\n",
        "          bboxs_gt\n",
        "        ).max(axis=1)[0].max(axis=0)[1]\n",
        "      ))\n",
        "      # index_positive = filter_iou(rois, bboxs_gt, min_iou=0.5)\n",
        "      index_negative = filter_iou(rois, bboxs_gt, min_iou=0.1, max_iou=0.5)\n",
        "\n",
        "      ## 2. Randomly sample a mini batch of size 64 in which we have up to\n",
        "      #     25% positive and 75% negative.\n",
        "      index_positive = index_positive[sample_bboxs(\n",
        "        rois[index_positive],\n",
        "        max_bboxs = 16\n",
        "      )]\n",
        "      debug_msg[\"positive_rois\"] = len(index_positive)\n",
        "\n",
        "      index_negative = index_negative[sample_bboxs(\n",
        "        rois[index_negative],\n",
        "        max_bboxs = 64 - index_positive.shape[0]\n",
        "      )]\n",
        "      debug_msg[\"negative_rois\"] = len(index_negative)\n",
        "\n",
        "      ## 3. Calculate index_positive_gt\n",
        "      index_positive_gt = torchvision.ops.box_iou(\n",
        "        rois[index_positive],\n",
        "        bboxs_gt\n",
        "      ).max(axis=1)[1]\n",
        "\n",
        "      ## 4. Calculate cls_loss\n",
        "      target = torch.hstack((\n",
        "        torch.zeros(index_negative.shape[0], dtype=torch.long, device=device),\n",
        "        labels_gt[index_positive_gt]    \n",
        "      ))\n",
        "      cls_score_training = torch.vstack((\n",
        "        cls_score[index_negative],\n",
        "        cls_score[index_positive]\n",
        "      ))\n",
        "      cls_loss = self.cls_loss(\n",
        "          cls_score_training,\n",
        "          target\n",
        "      )\n",
        "\n",
        "      ## 6. Calculate reg_loss\n",
        "      reg_loss = self.reg_loss(\n",
        "        detections_offset[index_positive][\n",
        "          torch.arange(len(index_positive), device=device),\n",
        "          labels_gt[index_positive_gt]\n",
        "        ],\n",
        "        get_bboxs_offset(\n",
        "          torchvision.ops.box_convert(\n",
        "            bboxs_gt[index_positive_gt],\n",
        "            in_fmt='xyxy',\n",
        "            out_fmt='cxcywh'\n",
        "          ),\n",
        "          rois[index_positive]\n",
        "        )    \n",
        "      )\n",
        "\n",
        "      return reg_loss, cls_loss, debug_msg\n",
        "    else:\n",
        "      ## 1. Obtain detections and labels with max cls score\n",
        "      scores, labels = cls_score.max(dim=1)\n",
        "      detections = get_bboxs(\n",
        "        detections_offset[\n",
        "          torch.arange(len(detections_offset), device=device),\n",
        "          labels\n",
        "        ],\n",
        "        rois\n",
        "      )\n",
        "\n",
        "      ## 2. Filter out detections with background as label\n",
        "      index = torch.arange(len(detections), device=device)[labels != 0]\n",
        "\n",
        "      ## 3. Apply class based NMS to filter out overlapping detections\n",
        "      index = index[torchvision.ops.batched_nms(\n",
        "        detections[index],\n",
        "        scores[index],\n",
        "        labels[index],\n",
        "        iou_threshold = 0.7\n",
        "      )]\n",
        "\n",
        "      # ## 4. Filter out detections that has probability below 0.8\n",
        "      # index = index[F.softmax(cls_score[index], dim=1).max(dim=1)[0] > 0.8]\n",
        "\n",
        "      ## 5. clip detections to image_boundaries\n",
        "      detections_clipped = torchvision.ops.clip_boxes_to_image(\n",
        "        detections[index],\n",
        "        image_boundaries\n",
        "      )\n",
        "\n",
        "      return detections_clipped, labels[index]"
      ],
      "metadata": {
        "id": "1mMW3xXv3PwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster RCNN"
      ],
      "metadata": {
        "id": "Cdt8vale3R1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.resnet50 = torchvision.models.resnet50(pretrained=True)\n",
        "    self.backbone = nn.Sequential(\n",
        "      self.resnet50.conv1,\n",
        "      self.resnet50.bn1,\n",
        "      self.resnet50.relu,\n",
        "      self.resnet50.maxpool,\n",
        "      self.resnet50.layer1,\n",
        "      self.resnet50.layer2,\n",
        "      self.resnet50.layer3\n",
        "    )\n",
        "\n",
        "    feature_map_channels = 1024\n",
        "    self.rpn = RPN(feature_map_channels)\n",
        "    self.rcnn = FastRCNN(feature_map_channels)\n",
        "    \n",
        "  def forward(self, img, gt=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      img: tensor of shape (1,C,H,W) representing 1 image\n",
        "      gt: A dict with keys bboxs_gt and labels_gt.\n",
        "        gt['bboxs_gt']: tensor of shape (M,4) in xyxy format representing\n",
        "          M ground truth bboxs on the image corresponding to the given\n",
        "          feature_map.\n",
        "        gt['labels_gt']: tensor of shape (M,) representing labels corresponding\n",
        "          to bboxs_gt.\n",
        "    \"\"\"\n",
        "    # 0. Set Mode\n",
        "    if self.training:\n",
        "      self.backbone.train()\n",
        "      self.rpn.train()\n",
        "      self.rcnn.train()\n",
        "    else:\n",
        "      self.backbone.eval()\n",
        "      self.rpn.eval()\n",
        "      self.rcnn.eval()\n",
        "    \n",
        "    # 1. Pre-Process\n",
        "    if self.training:\n",
        "      img, gt['bboxs_gt'] = self.preprocess(img, gt['bboxs_gt'])\n",
        "    else:\n",
        "      img = self.preprocess(img)\n",
        "    image_boundaries = img.shape[2:]\n",
        "\n",
        "    # 2. Pass img to resnet50 backbone\n",
        "    feature_map = self.backbone(img)\n",
        "\n",
        "    # 3. RPN forward pass\n",
        "    if self.training:\n",
        "      rois, rpn_reg_loss, rpn_cls_loss, rpn_debug_msg = self.rpn(\n",
        "        feature_map, image_boundaries, bboxs_gt=gt['bboxs_gt'])\n",
        "    else:\n",
        "      rois = self.rpn(feature_map, image_boundaries)\n",
        "\n",
        "    # 4. FastRCNN forward pass\n",
        "    if self.training:\n",
        "      rcnn_reg_loss, rcnn_cls_loss, rcnn_debug_msg = self.rcnn(\n",
        "        rois,\n",
        "        feature_map,\n",
        "        image_boundaries,\n",
        "        gt=gt\n",
        "      )\n",
        "    else:\n",
        "      detections, labels = self.rcnn(rois, feature_map, image_boundaries)\n",
        "\n",
        "    # 5. Return\n",
        "    if self.training:\n",
        "      return rpn_reg_loss, rpn_cls_loss, rcnn_reg_loss, rcnn_cls_loss, (rpn_debug_msg, rcnn_debug_msg)\n",
        "    else:\n",
        "      return detections, labels\n",
        "\n",
        "  def preprocess(self, img, bboxs_gt=None):\n",
        "    # 1. Resize the Image \n",
        "    _, _, H, W = img.shape\n",
        "    img = torchvision.transforms.Resize(600)(img)\n",
        "\n",
        "    # 2. Normalize the the image\n",
        "    normalize = torchvision.transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
        "    img = normalize(img)\n",
        "\n",
        "    # 3. Resize the bboxs according to image resize\n",
        "    if bboxs_gt is None:\n",
        "      return img\n",
        "    else:\n",
        "      _, _, H_, W_ = img.shape\n",
        "      bboxs_gt = bboxs_gt * torch.tensor([W_/W, H_/H, W_/W, H_/H], dtype=torch.float32, device=device)\n",
        "      return img, bboxs_gt"
      ],
      "metadata": {
        "id": "VwhpkvTU3YZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Pre-Process"
      ],
      "metadata": {
        "id": "NaT-7cbmYids"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FasterRCNN()\n",
        "\n",
        "img, meta = train_loader.dataset[0]\n",
        "bboxs, labels, ids = get_bboxs_and_labels(meta, from_loader=False)\n",
        "img = img.unsqueeze(0).to(device)\n",
        "bboxs = bboxs.to(device)\n",
        "ids = ids.to(device)\n",
        "\n",
        "disp(img[0], bboxs, labels)\n",
        "\n",
        "img_, bboxs_ = model.preprocess(img, bboxs_gt=bboxs)\n",
        "disp(img_[0], bboxs_, labels)"
      ],
      "metadata": {
        "id": "EOcC0RZFYqR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "DT_fnwZ5srr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title log_debug\n",
        "def log_debug(writer, debug_msg, i, epoch):\n",
        "  writer.add_scalar(\n",
        "    f\"Epoch{epoch}/RPN_rois\",\n",
        "    debug_msg[0][\"rois\"],\n",
        "    i\n",
        "  )\n",
        "  # writer.add_scalar(\n",
        "  #   f\"Epoch{epoch}/RPN_rois_after_filter_cross_boundary\",\n",
        "  #   debug_msg[0][\"rois_after_filter_cross_boundary\"],\n",
        "  #   i\n",
        "  # )\n",
        "  writer.add_scalar(\n",
        "    f\"Epoch{epoch}/RPN_positive_rois\",\n",
        "    debug_msg[0][\"positive_rois\"],\n",
        "    i\n",
        "  )\n",
        "  writer.add_scalar(\n",
        "    f\"Epoch{epoch}/RPN_negative_rois\",\n",
        "    debug_msg[0][\"negative_rois\"],\n",
        "    i\n",
        "  )\n",
        "  writer.add_scalar(\n",
        "    f\"Epoch{epoch}/RCNN_positive_rois\",\n",
        "    debug_msg[1][\"positive_rois\"],\n",
        "    i\n",
        "  )\n",
        "  writer.add_scalar(\n",
        "    f\"Epoch{epoch}/RCNN_negative_rois\",\n",
        "    debug_msg[1][\"negative_rois\"],\n",
        "    i\n",
        "  )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kBjX9c6fwIvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, writer, epoch):\n",
        "  model.train()\n",
        "  for i, (img, meta) in enumerate(tqdm(train_loader)):\n",
        "    bboxs, labels, ids = get_bboxs_and_labels(meta)\n",
        "\n",
        "    img = img.to(device)\n",
        "    bboxs = bboxs.to(device)\n",
        "    ids = ids.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    rpn_reg_loss, rpn_cls_loss, rcnn_reg_loss, rcnn_cls_loss, debug_msg\\\n",
        "      = model(img, {'bboxs_gt': bboxs, 'labels_gt': ids})\n",
        "\n",
        "    log_debug(writer, debug_msg, i, epoch)\n",
        "    writer.add_scalar(f'Epoch{epoch}/rpn_reg_loss', rpn_reg_loss, i)\n",
        "    writer.add_scalar(f'Epoch{epoch}/rpn_cls_loss', rpn_cls_loss, i)\n",
        "    writer.add_scalar(f'Epoch{epoch}/rcnn_reg_loss', rcnn_reg_loss, i)\n",
        "    writer.add_scalar(f'Epoch{epoch}/rcnn_cls_loss', rcnn_cls_loss, i)\n",
        "\n",
        "    loss = rpn_reg_loss + rpn_cls_loss + rcnn_reg_loss + rcnn_cls_loss\n",
        "\n",
        "    writer.add_scalar(f'Epoch{epoch}/total', loss, i)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def train_epochs(model, train_loader, train_args, writer):\n",
        "  lr = train_args[\"lr\"]\n",
        "  epochs = train_args[\"epochs\"]\n",
        "  model = model.to(device)\n",
        "  optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    train(model, train_loader, optimizer, writer, epoch)"
      ],
      "metadata": {
        "id": "QZtH3K_cszP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "77g54usW3dMB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGqI4Qb9-9u-"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter(\"runs/exp_1\")\n",
        "\n",
        "model = FasterRCNN()\n",
        "train_epochs(model, train_loader, {'lr': 1e-4, 'epochs': 4}, writer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "h8BGerFKFbTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "E73A-UWk3hzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(img, bboxs):\n",
        "  # 1. Resize the Image \n",
        "  _, H, W = img.shape\n",
        "  img = torchvision.transforms.Resize(600)(img)\n",
        "\n",
        "  # 2. Resize the bboxs according to image resize\n",
        "  _, H_, W_ = img.shape\n",
        "  bboxs = bboxs * torch.tensor([W_/W, H_/H, W_/W, H_/H], dtype=torch.float32)\n",
        "\n",
        "  return img, bboxs"
      ],
      "metadata": {
        "id": "Ko_or9TO7ZhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, meta = train_loader.dataset[30]\n",
        "bboxs, labels, ids = get_bboxs_and_labels(meta, from_loader=False)\n",
        "\n",
        "# disp(img, bboxs, labels)\n",
        "\n",
        "img_disp, bboxs_disp = resize(img, bboxs)\n",
        "img_disp = (img_disp * 255).to(torch.uint8)\n",
        "\n",
        "img = img.to(device)\n",
        "\n",
        "model.eval()\n",
        "detections, labels = model(img.unsqueeze(0))\n",
        "print(labels)\n",
        "\n",
        "detections_m = torchvision.ops.box_convert(detections, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "img_disp = torchvision.utils.draw_bounding_boxes(\n",
        "  img_disp,\n",
        "  detections_m[15:17],\n",
        "  colors = (0,0,255),\n",
        "  width=2)\n",
        "\n",
        "img_disp = torchvision.utils.draw_bounding_boxes(\n",
        "  img_disp,\n",
        "  bboxs_disp,\n",
        "  colors = (255,0,0),\n",
        "  width=2)\n",
        "\n",
        "plt.imshow(img_disp.permute(1,2,0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qCE61tZNyyL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3)\n",
        "print(detections)\n",
        "print(detections[:,2]-detections[:,0], detections[:,3]-detections[:,1])"
      ],
      "metadata": {
        "id": "Tfrib3fYR0M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, labels in enumerate(voc_labels):\n",
        "  print(f\"{i}:{labels}\")"
      ],
      "metadata": {
        "id": "vQXG-uIBKj15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Docs"
      ],
      "metadata": {
        "id": "fKlp9dQTcjmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster RCNN\n",
        "\n",
        "### PreProcess\n",
        "### Common\n",
        "> **Input:** image, `bboxs_gt`<br/>\n",
        "> **Output:** `image` (i.e. pre-processed image), `bboxs_gt` (i.e. pre-processed `bboxs_gt`)\n",
        "\n",
        "1. Normalize the input image with mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
        "2. Rescale the normalized image such that shorter side in 300 pixels and return `image`.\n",
        "3. Rescale the `bboxs_gt` according to the image rescale and return `bboxs_gt`.\n",
        "\n",
        "### Backbone\n",
        "### Common\n",
        "> **Input:** `image`<br/>\n",
        "> **Output:** `feature_map`, `subsampling_ratio`\n",
        "\n",
        "1. Pass `image` to resnet50 backbone (i.e. upto layer3) and return output as `feature_map`.\n",
        "2. if `image.shape` = (C,H,W) and `feature_map.shape` = (Cf,Hf,Wf) return `subsampling_ratio` as (Hf/H, Wf/W)\n",
        "\n",
        "### RPN\n",
        "#### Common\n",
        "> **Input:** `feature_map`<br/>\n",
        "> **Output:** `objectness_score`, `rois_offset`\n",
        "\n",
        "1. Pass `feature_map` to 3x3 convolution to reduce depth to 512 while conserving Hf and Wf.\n",
        "   - stride=1 and padding=1\n",
        "   - output: `feature_map` of shape (512, Hf, Wf)\n",
        "2. Pass above output to two 1x1 convolution to get `objectness_score` and `rois_offset`.\n",
        "   1. `objectness_score`: (2 x 9, Hf, Wf)\n",
        "     - Number of anchors: 9\n",
        "     - (object score + not object score) per anchors: 2\n",
        "   2. `rois_offset`: (4 x 9, Hf, Wf)\n",
        "     - Number of anchors: 9\n",
        "     - Number of param for roi offset of 1 anchors: 4\n",
        "3. Reshape `objectness_score` and `rois_offset` to `(Hf*Wf*9,2)` and `(Hf*Wf*9,4)` respectively.\n",
        "\n",
        "#### Training\n",
        "> **Input:** `image_boundaries`, `objectness_score`, `rois_offset`, `bboxs_gt`<br/>\n",
        "> **Output:** `cls_loss`, `reg_loss`\n",
        "\n",
        "1. Filter out rois which goes beyond the boundaries of `image`.\n",
        "2. Based on IoU (Intersection over Union) of rois with `bboxs_gt` classify rois into positive and negative.\n",
        "    1. **Positive:** if IoU > 0.5\n",
        "    2. **Negative:** if IoU < 0.1\n",
        "3. Sample 256 rois with `#rois_positive/#rois_negative` as close to 1 a possible.\n",
        "4. Calculate and return `cls_loss` as cross-entropy loss over objectness score with positive being object and negative not being object.\n",
        "5. Calculate `rois_positive_gt = [bbox_in_gt_with_maxIoU(roi_positive, bboxs_gt) for roi_positive in rois_positive]`.\n",
        "6. Calculate and return `reg_loss` as smooth L1 loss of (`rois_offset_positive` - `rois_offset_positive_gt`)\n",
        "\n",
        "#### Inference\n",
        "> **Input:** `image_boundaries`, `objectness_score`, `rois_offset`<br/>\n",
        "> **Output:** `rois`\n",
        "\n",
        "1. Sort rois based on objectness score.\n",
        "2. Filter out rois using NMS whith IoU threshold: 0.6\n",
        "3. Keep top N=500 proposals.\n",
        "4. Clip filtered rois to `image` boundaries and return the clipped rois.\n",
        "\n",
        "### FastRCNN\n",
        "#### Common\n",
        "> **Input:** rois, `feature_map`, `subsampling_ratio`<br/>\n",
        "> **Output:** `classification_score`, `detections_offset`\n",
        "\n",
        "1. Perform ROI Pooling with rois on `feature_map` and obtain `rois_fm` (rois feature map).\n",
        "2. pass `rois_fm` to two fully connected layer of dimension 512.\n",
        "3. Pass the output of above to two different fc layer:\n",
        "    1. 21 dimensional to get `classification_score`.\n",
        "    2. 21 * 4 dimensional to get `detections_offset`.\n",
        "\n",
        "#### Training\n",
        "> **Input:** `classification_score`, `detections_offset`, `bboxs_gt`, `labels_gt`<br/>\n",
        "> **Output:** `cls_loss`, `reg_loss`\n",
        "\n",
        "1. Based on IoU (Intersection over Union) of rois with `bboxs_gt` classify rois into positive and negative.\n",
        "    1. **Positive:** if IoU > 0.5\n",
        "    2. **Negative:** if 0.1 < IoU < 0.5\n",
        "2. Randomly sample a mini batch of size 64 in which we have up to 25% positive and 75% negative.\n",
        "3. Calculate\n",
        "    1. `detections_positive_gt_bboxs = [bbox_in_gt_with_maxIoU(detection_positive, bboxs_gt) for detection_positive in detections_positive]`\n",
        "    2. `detections_positive_gt_labels = [labels_with_maxIoU(detection_positive, bboxs_gt) for detection_positive in detections_positive]` \n",
        "5. Calculate and return `cls_loss` as cross-entropy loss over `classification_score` with positive being object with `detections_positive_gt_labels` as labels and negative being background.\n",
        "6. Calculate and return `reg_loss` as smooth L1 loss of (`detection_offset_positive` - `detection_offset_positive_gt_bboxs`)\n",
        "\n",
        "#### Inference\n",
        "> **Input:** `image_boundaries`, `classification_score`, `detections_offset`<br/>\n",
        "> **Output:** detections, labels\n",
        "\n",
        "1. Obtain detections with max classification score.\n",
        "2. Filter out detections in which background has max classification score.\n",
        "3. Apply class based NMS.\n",
        "    1. Group the detections by class\n",
        "    2. Sort them by probability.\n",
        "    3. Apply NMS to each group independently.\n",
        "4. Filter out detections that has probability below a certain threshold.\n",
        "5. clip detections to `image_boundaries`.\n",
        "\n",
        "### FasterRCNN\n",
        "#### Training\n",
        "> **Input:** image, `bboxs_gt`, `labels_gt`<br/>\n",
        "> **Output:** `rpn_cls_loss`, `rpn_reg_loss`, `rcnn_cls_loss`, `rcnn_reg_loss`\n",
        "\n",
        "1. `image, bboxs_gt = PreProcess.Common(image, bboxs_gt)`\n",
        "2. `feature_map, subsampling_ratio = Backbone.Common(image)`\n",
        "3. `objectness_score, rois_offset = RPN.Common(feature_map)`\n",
        "4. `rpn_cls_loss, rpn_reg_loss = RPN.Training(image.shape[2:], objectness_score, rois_offset, bboxs_gt)`\n",
        "5. `rois = RPN.Inference(image.shape[2:], objectness_score, rois_offset)`\n",
        "6. `classification_score, detections_offset = FastRCNN.Common(rois, feature_map, subsampling_ratio)`\n",
        "7. `rcnn_cls_loss, rcnn_reg_loss = FasterRCNN.Training(classification_score, detections_offset, bboxs_gt, labels_gt)`\n",
        "\n",
        "#### Inference\n",
        "> **Input:** image<br/>\n",
        "> **Output:** detections, labels\n",
        "\n",
        "1. `image, bboxs_gt = PreProcess.Common(image, bboxs_gt)`\n",
        "2. `feature_map, subsampling_ratio = Backbone.Common(image)`\n",
        "3. `objectness_score, rois_offset = RPN.Common(feature_map)`\n",
        "5. `rois = RPN.Inference(image.shape[2:], objectness_score, rois_offset)`\n",
        "6. `classification_score, detections_offset = FastRCNN.Common(rois, feature_map, subsampling_ratio)`\n",
        "7. `detections, labels = FastRCNN.Inference(image.shape[2:], classification, detections_offset)`"
      ],
      "metadata": {
        "id": "i_hJa4vYcWxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with torchvision pretrained FasterRCNN detection"
      ],
      "metadata": {
        "id": "NWRy0252PALk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter(\"runs/torchvision_exp_1\")\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "model.train()\n",
        "for i, (img, meta) in enumerate(tqdm(train_loader)):\n",
        "  bboxs, labels, ids = get_bboxs_and_labels(meta)\n",
        "\n",
        "  img = img.to(device)\n",
        "  bboxs = bboxs.to(device)\n",
        "  ids = ids.to(device)\n",
        "\n",
        "  target = [{\n",
        "    'boxes': bboxs,\n",
        "    'labels': ids\n",
        "  }]\n",
        "  output = model(img, target)\n",
        "\n",
        "  writer.add_scalar(f'Epoch1/rpn_cls_loss', output['loss_objectness'], i)\n",
        "  writer.add_scalar(f'Epoch1/rpn_reg_loss', output['loss_rpn_box_reg'], i)\n",
        "  writer.add_scalar(f'Epoch1/rcnn_cls_loss', output['loss_classifier'], i)\n",
        "  writer.add_scalar(f'Epoch1/rcnn_reg_loss', output['loss_box_reg'], i)\n",
        "  \n",
        "  loss = output['loss_objectness'] + output['loss_rpn_box_reg'] + output['loss_classifier'] + output['loss_box_reg']\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "aPH8tW4sPOUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "print(COCO_INSTANCE_CATEGORY_NAMES[72])\n",
        "print(COCO_INSTANCE_CATEGORY_NAMES[1])\n",
        "print(COCO_INSTANCE_CATEGORY_NAMES[44])\n",
        "print(COCO_INSTANCE_CATEGORY_NAMES[59])"
      ],
      "metadata": {
        "id": "VdIUOGYURKH8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PfxaoBAYX4A2",
        "kEr3zhg5YAPS",
        "MrpqtVAtXulR",
        "d2lBCqcRTzUG",
        "z_5uRi5tT7c6",
        "eh5esKnofA42",
        "pSfKejf1sEaD",
        "POH-O3vdsWWk",
        "Cdt8vale3R1O",
        "NaT-7cbmYids",
        "DT_fnwZ5srr8",
        "77g54usW3dMB",
        "fKlp9dQTcjmY",
        "NWRy0252PALk"
      ],
      "name": "FasterRCNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}